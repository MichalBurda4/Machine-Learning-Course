# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18nlp8900lI_VYF0qLuLlyvxt2tlql6oR

**1 Cel/Zakres**

• Klasteryzacja

• Znajdywanie parametrów dla algorytmów klasteryzacji.

**2 Przygotowanie danych**
```
from sklearn.datasets import fetch_openml
import numpy as np
mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')
mnist.target = mnist.target.astype(np.uint8)
X = mnist["data"]
y = mnist["target"]
```

**3 Ćwiczenie**

Pozyskane dane (zmienna X) reprezentują zeskanowane znaki nieznanego alfabetu ;). Celem
ćwiczenia jest identyfikacja ile tych znaków jest i jak mogą one wyglądać.
Zakładając, że możemy mieć do czynienia z 8–12 różnymi znakami uzyj metody centroidów do ich
klasteryzacji.
1. Przeprowadź klasteryzację dla 8, 9, 10, 11 i 12 skupisk za każdym razem wybierając najlepszy
z 10 modeli pod względem inercji (parametr n_init).
2. Wylicz wartośc wskaźnika sylwetkowego dla każdego z ww. skupisk. Zapisz wartość wszystkich wskaźników sylwetkowych jako listę w pliku Pickle o nazwie kmeans_sil.pkl.
5 pkt.
3. Znany lingwista prof. Talent twierdzi, że w zbiorze X można zidentyfikować 10 różnych
znaków. Czy wartości wskaźnika sylwetkowego potwierdzają tą obserwację?
4. Prof. Talent dostarczył swoich wyników klasyfikacji w postaci zbioru y. Policz macierz błędów
pomiędzy danymi otrzymanymi z procesu klasteryazacji dla 10 skupisk i zbioru y.
5. Dla każdego wiersza ww. macierzy znajdź indeks o najwyższej wartości (np. numpy.argmax()
albo pandas.Series.argmax()). Wartości umieść na posortowanej rosnąco liście bez duplikatów (użyj np. set()). Listę zapisz w pliku Pickle o nazwie kmeans_argmax.pkl.
1
2 pkt.
6. Znajdź sensowne wartości parametru eps dla DBSCAN. Heurystyka dla określenia wartości
parametru eps oparta jest o odległość euklidesową pomiędzy instancjami. Policz odległości
dla pierwszych 300 elementów ze zbioru X ze wszystkimi pozostałymi elementami w zbiorze
X (użyj np. numpy.linalg.norm(x1-x2), gdzie x1 i x2 to punkty w przestrzeni wielowymiarowej), pomiń odległości równe 0, a następnie wyświetl 10 najmniejszych. Ww. 10 wartości
umieść na liście w kolejności rosnącej, a listę zapisz w pliku Pickle o nazwie dist.pkl.
2 pkt.
7. Policz średnią s z 3 najmniejszych wartości z ww. listy. Przyjmij kolejno wartości eps od s
do s+10%*s z krokiem co 4%*s i wykonaj klasteryzacje.
8. Dla każdej klasteryzacji (dla kolejnych wartości eps) policz ile jest unikalnych etykiet zidentyfikowanych przez algorytm DBSCAN. Wartości umieść na liście i zapisz w pliku Pickle o
nazwie dbscan_len.pkl.
5 pkt.

Przygotowanie danych
"""

from sklearn.datasets import fetch_openml
import numpy as np
import pickle
from statistics import mode
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix
from sklearn.metrics import silhouette_score
from sklearn.cluster import DBSCAN

mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')
mnist.target = mnist.target.astype(np.uint8)
X = mnist["data"]
y = mnist["target"]

"""zad1 zad2 zad3 zad4

**zad1** Przeprowadź klasteryzację dla 8, 9, 10, 11 i 12 skupisk za każdym razem wybierając najlepszy z 10 modeli pod względem inercji (parametr n_init).

**zad2**
Wylicz wartośc wskaźnika sylwetkowego dla każdego z ww. skupisk. Zapisz wartość wszystkich wskaźników sylwetkowych jako listę w pliku Pickle o nazwie kmeans_sil.pkl. 5 pkt.

**zad3**
Znany lingwista prof. Talent twierdzi, że w zbiorze X można zidentyfikować 10 różnych znaków. Czy wartości wskaźnika sylwetkowego potwierdzają tą obserwację?

**zad4**
Prof. Talent dostarczył swoich wyników klasyfikacji w postaci zbioru y. Policz macierz błędów pomiędzy danymi otrzymanymi z procesu klasteryazacji dla 10 skupisk i zbioru y.
"""

silhouette_dictionary = {} #Słownik przechowująca wartości metryki Silhouette dla różnych wartości k

for i in range(8, 13):
    # Inicjalizacja modelu K-Means dla obecnej liczby klastrów i 10 inicjalizacji
    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)
    # Dopasowanie modelu do danych X
    kmeans.fit(X)
    silhouette_dictionary[i] = kmeans

#Wypisanie
for i, model in silhouette_dictionary.items():
    print(f"k= {i}: {model.inertia_}")

# Obliczenie wartości silhouette_score dla każdego modelu w słowniku silhouette_dictionary
silhouette_list = [silhouette_score(X, model.labels_) for n_clusters, model in silhouette_dictionary.items()]
#zapis do pliku
with open('kmeans_sil.pkl', 'wb') as file:
    pickle.dump(silhouette_list, file)

"""**zad5**

Dla każdego wiersza ww. macierzy znajdź indeks o najwyższej wartości (np. numpy.argmax() albo pandas.Series.argmax()). Wartości umieść na posortowanej rosnąco liście bez duplikatów (użyj np. set()). Listę zapisz w pliku Pickle o nazwie kmeans_argmax.pkl. 1 2 pkt.
"""

confusion_mat = confusion_matrix(y, silhouette_dictionary[10].labels_)
max_index = sorted(list(set(np.argmax(row) for row in confusion_mat)))

with open('kmeans_argmax.pkl', 'wb') as file:
    pickle.dump(max_index, file) # Zapisanie danych do pliku pickle

"""**zad6** Znajdź sensowne wartości parametru eps dla DBSCAN. Heurystyka dla określenia wartości parametru eps oparta jest o odległość euklidesową pomiędzy instancjami. Policz odległości dla pierwszych 300 elementów ze zbioru X ze wszystkimi pozostałymi elementami w zbiorze X (użyj np. numpy.linalg.norm(x1-x2), gdzie x1 i x2 to punkty w przestrzeni wielowymiarowej), pomiń odległości równe 0, a następnie wyświetl 10 najmniejszych. Ww. 10 wartości umieść na liście w kolejności rosnącej, a listę zapisz w pliku Pickle o nazwie dist.pkl. 2 pkt."""

# Obliczanie odległości dla pierwszych 300 elementów w X względem wszystkich innych elementów w X
odleglosci = np.array([np.linalg.norm(X[i] - X[j]) for i in range(300) for j in range(len(X))])

# Usunięcie odległości między tymi samymi elementami (odległość = 0)
odleglosci = [o for o in odleglosci if o != 0]

# Wybór 10 najmniejszych odległości
odleglosci_najmniejsze = np.sort(odleglosci)[:10]

# Zapisanie najmniejszych odległości do pliku binarnego za pomocą modułu pickle
with open("dist.pkl", "wb") as plik:
    pickle.dump(list(odleglosci_najmniejsze), plik)

# Wyświetlenie najmniejszych odległości
print("Najmniejsze odległości = ", odleglosci_najmniejsze)

"""**zad7**
Policz średnią s z 3 najmniejszych wartości z ww. listy. Przyjmij kolejno wartości eps od s do s+10%s z krokiem co 4%s i wykonaj klasteryzacje.
"""

# Obliczanie średniej wartości `s` z trzech najmniejszych odległości
s = np.mean(odleglosci_najmniejsze[:3])
print(f"Średnia wartość s z trzech najmniejszych odległości: {s:.2f}")

eps_min = s
eps_max = s + 0.10 * s
eps_step = 0.04 * s

eps_values = np.arange(eps_min, eps_max, eps_step)
print(f"Wartości eps: {eps_values}")

"""**zad 8** Dla każdej klasteryzacji (dla kolejnych wartości eps) policz ile jest unikalnych etykiet zidentyfikowanych przez algorytm DBSCAN. Wartości umieść na liście i zapisz w pliku Pickle o nazwie dbscan_len.pkl. 5 pkt."""

unique_labels = []

for eps in eps_values:
    dbscan = DBSCAN(eps=eps)
    dbscan.fit(X)

    unique_labels.append(len(set(dbscan.labels_)))

#zapis do pliku
with open('dbscan_len.pkl', 'wb') as file:
    pickle.dump(unique_list, file)

unique_labels