# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tgvfy5baBxYm-X8GJMcE0_R1W3Fm5gkK

**Laboratorium: Rekurencyjne sieci neuronowe**
"""

import pandas as pd
import pickle
import tensorflow as tf
import numpy as np
from sklearn.metrics import mean_absolute_error

"""**2 Ćwiczenia**

2.1 Pobieranie danych
```
tf.keras.utils.get_file(
"bike_sharing_dataset.zip",
"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip",
cache_dir=".",
extract=True
)
```

"""

tf.keras.utils.get_file(
"bike_sharing_dataset.zip",
"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip",
cache_dir=".",
extract=True
)

"""**2.2 Przygotowanie danych**"""

df = pd.read_csv('datasets/hour.csv',
parse_dates={'datetime': ['dteday', 'hr']},
date_format='%Y-%m-%d %H',
index_col='datetime'
)
print((df.index.min(), df.index.max()))
print((365 + 366) * 24 - len(df))
df

df.notna().sum()

df = df.asfreq('H')
df[['casual', 'registered', 'cnt']] = df[['casual', 'registered', 'cnt']].fillna(0)

df['temp'] = df['temp'].interpolate()
df['atemp'] = df['atemp'].interpolate()
df['hum'] = df['hum'].interpolate()
df['windspeed'] = df['windspeed'].interpolate()

df = df.fillna(method='ffill')

df.casual /= 1e3
df.registered /= 1e3
df.cnt /= 1e3
df.weathersit /= 4

df

print(df.notna().sum())
print(df[['casual', 'registered', 'cnt', 'weathersit']].describe())


df_2weeks = df[:24 * 7 * 2]
df_2weeks[['casual', 'registered', 'cnt', 'temp']].plot(figsize=(10, 3))


df_daily = df.resample('W').mean()
df_daily[['casual', 'registered', 'cnt', 'temp']].plot(figsize=(10, 3))

"""**2.3 Wskaźniki bazowe**"""

mae_daily = df['cnt'].diff(24).abs().mean() * 1e3
mae_weekly = df['cnt'].diff(24 * 7).abs().mean() * 1e3

mae_baseline = (mae_daily, mae_weekly)
print(mae_baseline)
with open('mae_baseline.pkl', 'wb') as f:
    pickle.dump(mae_baseline, f)

"""**2.4 Predykcja przy pomocy sieci gęstej**"""

cnt_train = df['cnt']['2011-01-01 00:00':'2012-06-30 23:00']
cnt_valid = df['cnt']['2012-07-01 00:00':]

seq_len = 1 * 24
train_ds = tf.keras.utils.timeseries_dataset_from_array(
cnt_train.to_numpy(),
targets=cnt_train[seq_len:],
sequence_length=seq_len,
batch_size=32,
shuffle=True,
seed=42
)
valid_ds = tf.keras.utils.timeseries_dataset_from_array(
cnt_valid.to_numpy(),
targets=cnt_valid[seq_len:],
sequence_length=seq_len,
batch_size=32
)

number_of_epochs = 20

"""**2.5 Prosta sieć rekurencyjna**

"""

model = tf.keras.Sequential([
tf.keras.layers.Dense(1, input_shape=[seq_len])
])
# Compile the model
model.compile(loss=tf.keras.losses.Huber(), optimizer='sgd', metrics=['mae'])
model.summary()
model.fit(train_ds, epochs=number_of_epochs, validation_data=valid_ds)

# Save the trained model to a file
model.save('model_linear.keras')

metrics = model.evaluate(valid_ds)

metrics = (metrics[1] * 1e3,)
print(metrics)

with open('mae_linear.pkl', 'wb') as f:
    pickle.dump(metrics, f)

model = tf.keras.Sequential([
tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])
])

model.compile(loss=tf.keras.losses.Huber(), optimizer='sgd', metrics=['mae'])
model.summary()
model.fit(train_ds, epochs=number_of_epochs, validation_data=valid_ds)
model.save('model_rnn1.keras')

metrics = model.evaluate(valid_ds)

metrics = (metrics[1] * 1e3,)
print(metrics)
with open('mae_rnn1.pkl', 'wb') as f:
    pickle.dump(metrics, f)

model = tf.keras.Sequential([
tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),
tf.keras.layers.Dense(1)
])

model.compile(loss=tf.keras.losses.Huber(), optimizer='sgd', metrics=['mae'])
model.summary()
model.fit(train_ds, epochs=number_of_epochs, validation_data=valid_ds)
model.save('model_rnn32.keras')

metrics = model.evaluate(valid_ds)

metrics = (metrics[1] * 1e3,)

with open('mae_rnn32.pkl', 'wb') as f:
    pickle.dump(metrics, f)

"""**2.6 Głęboka RNN**

Rozbuduj model tak, aby zawierał kilka (np. 3) warstwy rekurencyjne. Pamiętaj o tym, aby wszystkie warstwy RNN oprócz ostatniej przekazywały dalej sekwencje (argument return_sequences).
Przeprowadź procedurę uczenia dla nowego modelu (20 epok). Zapisz przyuczony model w pliku
model_rnn_deep.keras.
3 p.
Oblicz uzyskaną wartość MAE dla zbioru walidacyjnego jako krotkę 1-elementową (mae_rnn_deep)
w pliku mae_rnn_deep.pkl. Pamiętaj o skalowaniu wartości.
1 p.
"""

model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),
    tf.keras.layers.SimpleRNN(32, return_sequences=True),
    tf.keras.layers.SimpleRNN(32),
    tf.keras.layers.Dense(1)
])

model.compile(loss=tf.keras.losses.Huber(), optimizer='sgd', metrics=['mae'])
model.summary()
model.fit(train_ds, epochs=number_of_epochs, validation_data=valid_ds)
model.save('model_rnn_deep.keras')

metrics = model.evaluate(valid_ds)

metrics = (metrics[1] * 1e3,)
with open('mae_rnn_deep.pkl', 'wb') as f:
    pickle.dump(metrics, f)

"""**2.7 Model wielowymiarowy**

Dotychczas dokonywaliśmny predykcji liczby wypożyczeń wyłącznie na podstawie poprzednich
wartości tego parametru. Ale przecież nasz zbiór danych zawiera też inne parametry, które mogą
wpływać na intensywność korzystania z rowerów miejskich.
Przygotuj zbiór danych dla sieci wielowariantowej, który w zbiorze cech, oprócz liczby wypożyczeń,
będzie zawierał również:

• sytuację pogodową,

• temperaturę odczuwalną,

• informację, czy dzień jest wolny czy roboczy.

6
Podziel zbiór tak jak poprzednio (18/6 miesięcy) i przygotuj zbiory danych przy pomocy
timeseries_dataset_from_array.
Utwórz model zawerający jedną warstwę RNN z 32 neuronami, ale dostosowaną do nowego kształtu
danych.
Przeprowadź procedurę uczenia dla nowego modelu przez 20 epok. Zapisz przyuczony model w
pliku model_rnn_mv.keras.
3 p.
Oblicz uzyskaną wartość MAE dla zbioru walidacyjnego jako krotkę 1-elementową (mae_rnn_mv)
w pliku mae_rnn_mv.pkl. Pamiętaj o przeskalowaniu wartości z powrotem do rzeczywistych jednostek.
1 p
"""

features = df[['cnt', 'weathersit', 'atemp', 'workingday']]
train_mv = features['2011-01-01 00:00':'2012-06-30 23:00']
valid_mv = features['2012-07-01 00:00':]

seq_len = 24
train_ds_mv = tf.keras.utils.timeseries_dataset_from_array(
    train_mv.to_numpy(),
    targets=train_mv['cnt'][seq_len:],
    sequence_length=seq_len,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_ds_mv = tf.keras.utils.timeseries_dataset_from_array(
    valid_mv.to_numpy(),
    targets=valid_mv['cnt'][seq_len:],
    sequence_length=seq_len,
    batch_size=32
)

model_rnn_mv = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 4]),
    tf.keras.layers.Dense(1)
])

model_rnn_mv.compile(
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
    loss=tf.keras.losses.Huber(),
    metrics=['mae']
)

history = model_rnn_mv.fit(train_ds_mv, validation_data=valid_ds_mv, epochs=20)

model_rnn_mv.save('model_rnn_mv.keras')

preds_mv = model_rnn_mv.predict(valid_ds_mv)
true_values_mv = np.concatenate([y.numpy() for x, y in valid_ds_mv])
mae_rnn_mv = mean_absolute_error(true_values_mv, preds_mv)

with open('mae_rnn_mv.pkl', 'wb') as f:
    pickle.dump((mae_rnn_mv,), f)

"""**3 Wyślij rozwiązanie**

Skrypt realizujący powyższe punkty zapisz w pliku lab12/lab12.py w swoim repozytorium.

"""